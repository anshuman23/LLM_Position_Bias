{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import json\n",
    "import requests\n",
    "import pickle as pkl\n",
    "from collections import defaultdict\n",
    "from googlesearch import search\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('azure-configuration.json') as inputfile:\n",
    "    azureconfig = json.load(inputfile)\n",
    "openai.api_key = azureconfig['key'] \n",
    "openai.api_base = azureconfig['endpoint'] \n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15' # this may change in the future\n",
    "\n",
    "deployment_name= azureconfig['deployment_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deployment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check if it's working\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-05-15\" \n",
    "\n",
    "print(deployment_name)\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=deployment_name, # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who were the founders of Microsoft?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message={\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_response_tokens = 500\n",
    "token_limit = 4096\n",
    "conversation = []\n",
    "conversation.append(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages):\n",
    "    encoding= tiktoken.get_encoding(\"cl100k_base\")  #model to encoding mapping https://github.com/openai/tiktoken/blob/main/tiktoken/model.py\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        # print(message)\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_CNN(article):\n",
    "\n",
    "    prompt=\"\"\"\n",
    "\n",
    "For the following article: {} \n",
    "\n",
    "Return a summary comprising of 3 sentences. With each sentence in a dash bulleted format.\n",
    "\n",
    "\"\"\".format(article)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(article):\n",
    "\n",
    "    prompt=\"\"\"\n",
    "\n",
    "For the following article: {} \n",
    "\n",
    "Return a summary comprising of 1 sentence. With the sentence in a dash bulleted format.\n",
    "\n",
    "\"\"\".format(article)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_output_parse(output):\n",
    "    first_split=output.split('\\n')\n",
    "    sentences=[]\n",
    "    for items in first_split:\n",
    "        sentences.append(items.split('-')[-1].strip())\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deployment_stuff():\n",
    "    with open('azure-configuration.json') as inputfile:\n",
    "        azureconfig = json.load(inputfile)\n",
    "    openai.api_key = azureconfig['key'] \n",
    "    openai.api_base = azureconfig['endpoint'] \n",
    "    openai.api_type = 'azure'\n",
    "    openai.api_version = '2023-05-15' # this may change in the future\n",
    "\n",
    "    deployment_name= azureconfig['deployment_name']\n",
    "\n",
    "    return deployment_name\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(deployment_name, article, name):\n",
    "    #Right now calling requests to get the synopsis, once Dataset is ready, replace it\n",
    "    #context=get_synopsis(series, episode, context)\n",
    "\n",
    "    #OpenAI Stuff\n",
    "     system_message={\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "     max_response_tokens = 4096\n",
    "     token_limit = 2000\n",
    "     conversation = []\n",
    "     conversation.append(system_message)\n",
    "\n",
    "     #print(len(article))\n",
    "     if name=='cnn_dailymail':\n",
    "         user_input = prompt_CNN(article)\n",
    "\n",
    "     else:\n",
    "         user_input = prompt(article)\n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # print(f\"Adword was {adword}\")\n",
    "    \n",
    "     conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    #conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "    # print(conv_history_tokens)\n",
    "\n",
    "    # while conv_history_tokens + max_response_tokens >= token_limit:\n",
    "    #     del conversation[0] \n",
    "    #     conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "\n",
    "     try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "        engine=deployment_name, # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "        messages=conversation,\n",
    "        temperature=0.7,\n",
    "        max_tokens=max_response_tokens,\n",
    "    )\n",
    "     except Exception as e:\n",
    "        #  print('In Exception')\n",
    "        #  print(e)\n",
    "         return []\n",
    "     \n",
    "         \n",
    "\n",
    "    # print(response['choices'][0]['message']['content'])\n",
    "\n",
    "     try:   \n",
    "        conversation.append({\"role\": \"assistant\", \"content\": response['choices'][0]['message']['content']})\n",
    "     except Exception as e:\n",
    "        #  print('In Exception')\n",
    "        #  print(e)\n",
    "         return []\n",
    "         \n",
    "\n",
    "    #print(conversation)\n",
    "\n",
    "     responses=prompt_output_parse(response['choices'][0]['message']['content'])\n",
    "\n",
    "     #print(responses)\n",
    "\n",
    "    # final_res.append(responses)\n",
    "\n",
    "    # if not responses:\n",
    "    #     #return empty\n",
    "    #     return None\n",
    "\n",
    "\n",
    "     return responses\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_name=deployment_stuff()\n",
    "# print(dep_name)\n",
    "\n",
    "# print(series_title[0])\n",
    "# print(episode_title[0])\n",
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "article_key = 'article'\n",
    "summary_key = 'highlights'\n",
    "name='cnn_dailymail'\n",
    "dataset=dataset['test']\n",
    "\n",
    "\n",
    "\n",
    "res=[]\n",
    "for article in tqdm(dataset[article_key]):\n",
    "    res.append(call(deployment_name, article, name))\n",
    "\n",
    "with open('cnn.pkl', 'wb') as f:\n",
    "  \n",
    "    pkl.dump(res,f)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"xsum\")\n",
    "article_key = 'document'\n",
    "summary_key = 'summary'\n",
    "dataset=dataset['test']\n",
    "\n",
    "\n",
    "name='xsum'\n",
    "\n",
    "\n",
    "\n",
    "res=[]\n",
    "for article in tqdm(dataset[article_key]):\n",
    " \n",
    "    res.append(call(deployment_name, article, name))\n",
    "\n",
    "with open('xsum.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "dataset = load_dataset(\"argilla/news-summary\")\n",
    "article_key = 'text'\n",
    "summary_key = 'prediction'\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['test'],\n",
    "        'test': dataset['train']})\n",
    "dataset=dataset['test']\n",
    "\n",
    "name='argilla/news-summary'\n",
    "\n",
    "\n",
    "res=[]\n",
    "for article in tqdm(dataset[article_key]):\n",
    " \n",
    "    res.append(call(deployment_name, article, name))\n",
    "\n",
    "with open('news.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset('reddit_tifu', 'long')\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "\n",
    "dataset=dataset['test']\n",
    "\n",
    "name='reddit_tifu'\n",
    "\n",
    "res=[]\n",
    "for article in tqdm(dataset[article_key]):\n",
    "  \n",
    "    res.append(call(deployment_name, article, name))\n",
    "\n",
    "\n",
    "with open('reddit.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "# with open(\"alternate_names.pkl\", 'wb') as f:\n",
    "#     pkl.dump(final_res,f)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_new_new.pkl', 'rb') as f:\n",
    "  \n",
    "    res=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4214"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for re in res:\n",
    "    if re:\n",
    "        count+=1\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_name=deployment_stuff()\n",
    "# print(dep_name)\n",
    "\n",
    "# print(series_title[0])\n",
    "# print(episode_title[0])\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "article_key = 'article'\n",
    "summary_key = 'highlights'\n",
    "name='cnn_dailymail'\n",
    "dataset=dataset['test']\n",
    "\n",
    "with open('cnn.pkl', 'rb') as f:\n",
    "    prev=pkl.load(f)\n",
    "\n",
    "res=[]\n",
    "for article, pre in tqdm(zip(dataset[article_key],prev)):\n",
    "    if pre:\n",
    "        res.append(pre)\n",
    "    else:\n",
    "        res.append(call(deployment_name, article, name))\n",
    "\n",
    "with open('cnn_new.pkl', 'wb') as f:\n",
    "  \n",
    "    pkl.dump(res,f)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"xsum\")\n",
    "article_key = 'document'\n",
    "summary_key = 'summary'\n",
    "dataset=dataset['test']\n",
    "\n",
    "\n",
    "name='xsum'\n",
    "\n",
    "with open('xsum.pkl', 'rb') as f:\n",
    "    prev=pkl.load(f)\n",
    "\n",
    "res=[]\n",
    "for article, pre in tqdm(zip(dataset[article_key],prev)):\n",
    "    if pre:\n",
    "        res.append(pre)\n",
    "    else:\n",
    "        res.append(call(deployment_name, article, name))\n",
    " \n",
    "\n",
    "with open('xsum_new.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"argilla/news-summary\")\n",
    "article_key = 'text'\n",
    "summary_key = 'prediction'\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['test'],\n",
    "        'test': dataset['train']})\n",
    "dataset=dataset['test']\n",
    "\n",
    "name='argilla/news-summary'\n",
    "\n",
    "with open('news.pkl', 'rb') as f:\n",
    "    prev=pkl.load(f)\n",
    "\n",
    "\n",
    "res=[]\n",
    "for article, pre in tqdm(zip(dataset[article_key],prev)):\n",
    "    if pre:\n",
    "        res.append(pre)\n",
    "    else:\n",
    "        res.append(call(deployment_name, article, name))\n",
    "\n",
    "with open('news_new.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset('reddit_tifu', 'long')\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "\n",
    "dataset=dataset['test']\n",
    "\n",
    "name='reddit_tifu'\n",
    "\n",
    "\n",
    "with open('reddit.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "res=[]\n",
    "for article, pre in tqdm(zip(dataset[article_key],prev)):\n",
    "    if pre:\n",
    "        res.append(pre)\n",
    "    else:\n",
    "        res.append(call(deployment_name, article, name))\n",
    "\n",
    "with open('reddit_new.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "# with open(\"alternate_names.pkl\", 'wb') as f:\n",
    "#     pkl.dump(final_res,f)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4214/4214 [13:47:23<00:00, 11.78s/it]      \n",
      "1000it [17:46,  1.07s/it]\n",
      "11334it [20:51:13,  6.62s/it]  \n",
      "11490it [3:27:11,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "dep_name=deployment_stuff()\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset('reddit_tifu', 'long')\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "\n",
    "dataset=dataset['test']\n",
    "\n",
    "name='reddit_tifu'\n",
    "\n",
    "res=[]\n",
    "for article in tqdm(dataset[article_key]):\n",
    "    sleep(1)\n",
    "    res.append(call(deployment_name, article, name))\n",
    "\n",
    "with open('reddit_new.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"argilla/news-summary\")\n",
    "article_key = 'text'\n",
    "summary_key = 'prediction'\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['test'],\n",
    "        'test': dataset['train']})\n",
    "dataset=dataset['test']\n",
    "\n",
    "name='argilla/news-summary'\n",
    "\n",
    "with open('news_new.pkl', 'rb') as f:\n",
    "    prev=pkl.load(f)\n",
    "\n",
    "\n",
    "res=[]\n",
    "for article, pre in tqdm(zip(dataset[article_key],prev)):\n",
    "    if pre:\n",
    "        res.append(pre)\n",
    "    else:\n",
    "        res.append(call(deployment_name, article, name))\n",
    "        sleep(1)\n",
    "\n",
    "with open('news_new_new.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"xsum\")\n",
    "article_key = 'document'\n",
    "summary_key = 'summary'\n",
    "dataset=dataset['test']\n",
    "\n",
    "\n",
    "name='xsum'\n",
    "\n",
    "with open('xsum_new.pkl', 'rb') as f:\n",
    "    prev=pkl.load(f)\n",
    "\n",
    "res=[]\n",
    "for article, pre in tqdm(zip(dataset[article_key],prev)):\n",
    "    sleep(1)\n",
    "    if pre:\n",
    "        res.append(pre)\n",
    "    else:\n",
    "        res.append(call(deployment_name, article, name))\n",
    "        sleep(1)\n",
    " \n",
    "\n",
    "with open('xsum_new_new.pkl', 'wb') as f:\n",
    "    pkl.dump(res,f)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "article_key = 'article'\n",
    "summary_key = 'highlights'\n",
    "name='cnn_dailymail'\n",
    "dataset=dataset['test']\n",
    "\n",
    "with open('cnn_new.pkl', 'rb') as f:\n",
    "    prev=pkl.load(f)\n",
    "\n",
    "res=[]\n",
    "for article, pre in tqdm(zip(dataset[article_key],prev)):\n",
    "    if pre:\n",
    "        res.append(pre)\n",
    "    else:\n",
    "        res.append(call(deployment_name, article, name))\n",
    "        sleep(1)\n",
    "\n",
    "with open('cnn_new_new.pkl', 'wb') as f:\n",
    "  \n",
    "    pkl.dump(res,f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_new_new.pkl', 'wb') as f:\n",
    "  \n",
    "    pkl.dump(res,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4214it [28:55,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "dep_name=deployment_stuff()\n",
    "\n",
    "dataset = load_dataset('reddit_tifu', 'long')\n",
    "\n",
    "\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "\n",
    "dataset=dataset['test']\n",
    "\n",
    "name='reddit_tifu'\n",
    "\n",
    "with open('reddit_new.pkl', 'rb') as f:\n",
    "    prev=pkl.load(f)\n",
    "\n",
    "\n",
    "res=[]\n",
    "for article, pre in tqdm(zip(dataset[article_key],prev)):\n",
    "    if pre:\n",
    "        res.append(pre)\n",
    "    else:\n",
    "        res.append(call(deployment_name, article, name))\n",
    "        sleep(1)\n",
    "\n",
    "with open('reddit_new_new.pkl', 'wb') as f:\n",
    "  \n",
    "    pkl.dump(res,f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
