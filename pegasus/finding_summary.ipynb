{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import evaluate\n",
    "import multiprocessing\n",
    "nltk.download('punkt')\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "import math, re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments, pipeline\n",
    "import argparse\n",
    "import pickle\n",
    "from rouge_score import rouge_scorer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('reddit_tifu', 'long')\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "    # 80% train, 20% test + validation\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well this was around 9 years ago when i was a 2nd grader. so basically it was some kids birthday and since it was primary school most kids brought cupcakes and the teacher would pass it out to the class. me being the hungry little shit that i was i was desperately in the mood for some vanilla cupcakes but the teacher decided to pass out the chocolate ones first. \\n\\nafter she had finished passing out the chocolate ones she picks up the vanilla ones and says, \"raise your hand if you want vanilla!\". at that moment i was talking to a friend and it took me a bit to process the fact that she just asked who wanted vanilla. i proceed to raise my hand and it turns out the cupcakes were right above me. i knocked the cupcakes out of her hand and they spilled  all over her. she got bat-shit angry and begins to make a scene in the middle of the whole cafeteria. she explained how i \"ruined her brand new shoes\" and the bitch even asked me if i had an allowance so i could pay for her shoes. \\n\\nshe had asked me at the time if i was messing around and i said yes because i literally didn\\'t know what else to say. after numerous emails from my mom to the teacher things finally cleared up and all was back to normal.\\n\\n**'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk(\"saved_models/pegasus-reddit-reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping_gen\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
      "mapping\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "mapping_gen\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "mapping\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "mapping_gen\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "mapping\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "mapping_gen\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "mapping\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "mapping_gen\n",
      "[0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
      "mapping\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "mapping_gen\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "mapping\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "mapping_gen\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "mapping\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "cumm_list1 = [0]*10\n",
    "cumm_list2 = [0]*10\n",
    "\n",
    "count=0\n",
    "for da, art in zip(data,dataset):\n",
    "    y1 = da['mapping_gen']\n",
    "    print('mapping_gen')\n",
    "    print(y1)\n",
    "    # cumm_list1 = [a+b for a,b in zip(cumm_list1, y1)]\n",
    "    y2 = da['mapping']\n",
    "    print('mapping')\n",
    "    print(y2)\n",
    "    count+=1\n",
    "    print(count)\n",
    "    input()\n",
    "    # cumm_list2 = [a+b for a,b in zip(cumm_list2, y2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
