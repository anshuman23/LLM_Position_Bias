{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/haskari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import evaluate\n",
    "import multiprocessing\n",
    "nltk.download('punkt')\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "import math, re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments, pipeline\n",
    "import argparse\n",
    "import pickle\n",
    "from rouge_score import rouge_scorer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "article_key = 'article'\n",
    "summary_key = 'highlights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"xsum\")\n",
    "article_key = 'document'\n",
    "summary_key = 'summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('reddit_tifu', 'long')\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "    # 80% train, 20% test + validation\n",
    "train_testvalid = data['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "data = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"argilla/news-summary\")\n",
    "article_key = 'text'\n",
    "summary_key = 'prediction'\n",
    "data = DatasetDict({\n",
    "    'train': data['test'],\n",
    "    'test': data['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 1.0,\n",
       "  'text': 'Highlights: The Trump presidency on March 31 at 6:19 p.m. EDT'}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[summary_key][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    example[\"original_article\"] = example[article_key]\n",
    "    example[\"article\"] = nltk.sent_tokenize(example[article_key])\n",
    "    \n",
    "    # # if args.dataset == 'news':\n",
    "    # check=example[summary_key][0]['text']\n",
    "    # example[\"highlights\"] = nltk.sent_tokenize(check)\n",
    "    # else:\n",
    "    # example[\"highlights\"] = nltk.sent_tokenize(example[summary_key])\n",
    "        \n",
    "    return example\n",
    "\n",
    "data = data.map(tokenize, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent', 'id', 'metadata', 'status', 'event_timestamp', 'metrics', 'original_article', 'article'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column highlights not in the dataset. Current columns in the dataset: ['text', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent', 'id', 'metadata', 'status', 'event_timestamp', 'metrics', 'original_article', 'article']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m sent_count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m      2\u001b[0m overall_count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m tqdm(data[\u001b[39m'\u001b[39;49m\u001b[39mhighlights\u001b[39;49m\u001b[39m'\u001b[39;49m]):\n\u001b[1;32m      5\u001b[0m     sent_count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(item)\n\u001b[1;32m      6\u001b[0m     overall_count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/acl/lib/python3.11/site-packages/datasets/arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/acl/lib/python3.11/site-packages/datasets/arrow_dataset.py:2787\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2786\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2787\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2788\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2789\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2790\u001b[0m )\n\u001b[1;32m   2791\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/miniconda3/envs/acl/lib/python3.11/site-packages/datasets/formatting/formatting.py:580\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    578\u001b[0m     _raise_bad_key_type(key)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 580\u001b[0m     _check_valid_column_key(key, table\u001b[39m.\u001b[39;49mcolumn_names)\n\u001b[1;32m    581\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/miniconda3/envs/acl/lib/python3.11/site-packages/datasets/formatting/formatting.py:520\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_valid_column_key\u001b[39m(key: \u001b[39mstr\u001b[39m, columns: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m columns:\n\u001b[0;32m--> 520\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[39m{\u001b[39;00mcolumns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column highlights not in the dataset. Current columns in the dataset: ['text', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent', 'id', 'metadata', 'status', 'event_timestamp', 'metrics', 'original_article', 'article']\""
     ]
    }
   ],
   "source": [
    "sent_count=0\n",
    "overall_count=0\n",
    "\n",
    "for item in tqdm(data['highlights']):\n",
    "    sent_count+=len(item)\n",
    "    overall_count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2340571.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sent_count=0\n",
    "overall_count=0\n",
    "min=float('inf')\n",
    "art=0\n",
    "for item in tqdm(data['article']):\n",
    "    #print(item)\n",
    "    sent_count=len(item)\n",
    "    if sent_count==0:\n",
    "        continue\n",
    "    if sent_count<min:\n",
    "        min=sent_count\n",
    "        art=item\n",
    "\n",
    "print(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WASHINGTON (Reuters) - Democratic Party communications director Luis Miranda is expected to leave the Democratic National Committee on Tuesday amid fallout from the party’s email hacking scandal, a source familiar with the matter told Reuters.']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_count/overall_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=0\n",
    "counts=[]\n",
    "for item in dataset[summary_key]:\n",
    "    # item=item[0]['text']\n",
    "    splits=item.split('\\n')\n",
    "    counts.append(len(splits))\n",
    "    if len(splits)>max_len:\n",
    "        max_len=max(len(splits),max_len)\n",
    "        print(splits)\n",
    "print(max_len)\n",
    "print(sum(counts)/len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[summary_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU \\\n",
    "    transformers==4.31.0 \\\n",
    "    accelerate==0.21.0 \\\n",
    "    einops==0.6.1 \\\n",
    "    langchain==0.0.240 \\\n",
    "    xformers==0.0.20 \\\n",
    "    bitsandbytes==0.41.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
